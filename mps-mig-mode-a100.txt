Argonne MPS/MIG Mode on A100
https://www.youtube.com/watch?v=nYI0yt3Q82Y&list=PLcbxjEfgjpO8HFNS3SHeTCY9bz9SkMNQs&index=5

MIG User Guide
https://docs.nvidia.com/datacenter/testla/mig-user-guide

MIG Partition editor for NVIDIA GPUs (nvidia-mig-parted)
https://github.com/nvidia/mig-parted

# Declaratively define the set of possible MIG partitions you want to create on GPUs
throughout your cluster or on a given node

$ nvidia-mig-parted apply -f config.yaml -c all-2g.10gb


Reference
https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/
https://developer.nvidia.com/blog/dividing-nvidia-a30-gpus-and-conquering-multiple-workloads/
https://developer.nvidia.com/blog/minimizing-dl-inference-latency-with-mig/
https://www.nvidia.com/en-us/on-demand/session/gtcfall20-a21657/
https://developer.nvidia.com/gtc/2020/video/s21975
https://www.youtube.com/watch?v=4ALztZDlkJ0
https://www.youtube.com/watch?v=xNY9cbaLuGk
https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf

https://github.com/aymericdamien/Tensorflow-Examples  Tensorflow Examples

Hands-On

Step (0)

Stop all running process using the GPU

By default MIG mode is not enabled on the GPU

$ sudo nvidia-smi -i 0
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-SXM4-40GB      Off  | 00000000:36:00.0 Off |                    0 |
| N/A   29C    P0    62W / 400W |      0MiB / 40537MiB |      6%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+

MIG mode can be enabled on a GPU with the command

$ sudo nvidia-smi -i 0 -mig 1
Enabled MIG Mode for GPU 00000000:36:00.0
All done.

$ sudo nvidia-smi -i 0 --query-gpu=pci.bus_id,mig.mode.current --format=csv
pci.bus_id, mig.mode.current
00000000:36:00.0, Enabled

List all enabled MIG GPU devices

$ sudo nvidia-smi \
--query-gpu=pci.bus_id,mig.mode.current \
--format=csv
pci.bus_id, mig.mode.current
00000000:36:00.0, Enabled
00000000:3B:00.0, Enabled
00000000:41:00.0, Enabled
00000000:45:00.0, Enabled
00000000:59:00.0, Enabled
00000000:5D:00.0, Enabled
00000000:63:00.0, Enabled
00000000:67:00.0, Enabled


Step (1)

$ sudo nvidia-smi 

Check if MIG mode is enabled with Nvidia-Smh this should list the GPUs followed by a table of MIG devices.
To see the list of possible GPU instances, use (what partitions I can do)

$ sudo nvidia-smi mig -lgip

For a specific GPU, pass its id with -i option for GPU 0 use. (what partitions I can do)

$ sudo nvidia-smi mig -i 0 -lgip

+-----------------------------------------------------------------------------+
| GPU instance profiles:                                                      |
| GPU   Name             ID    Instances   Memory     P2P    SM    DEC   ENC  |
|                              Free/Total   GiB              CE    JPEG  OFA  |
|=============================================================================|
|   0  MIG 1g.5gb        19     7/7        4.75       No     14     0     0   |
|                                                             1     0     0   |
+-----------------------------------------------------------------------------+
|   0  MIG 1g.5gb+me     20     1/1        4.75       No     14     1     0   |
|                                                             1     1     1   |
+-----------------------------------------------------------------------------+
|   0  MIG 1g.10gb       15     4/4        9.62       No     14     1     0   |
|                                                             1     0     0   |
+-----------------------------------------------------------------------------+
|   0  MIG 2g.10gb       14     3/3        9.62       No     28     1     0   |
|                                                             2     0     0   |
+-----------------------------------------------------------------------------+
|   0  MIG 3g.20gb        9     2/2        19.50      No     42     2     0   |
|                                                             3     0     0   |
+-----------------------------------------------------------------------------+
|   0  MIG 4g.20gb        5     1/1        19.50      No     56     2     0   |
|                                                             4     0     0   |
+-----------------------------------------------------------------------------+
|   0  MIG 7g.40gb        0     1/1        39.25      No     98     5     0   |
|                                                             7     1     1   |
+-----------------------------------------------------------------------------+
                                                                         
Placements in how many ways you can split your gpu (See the placements choices use lgipp option

$ sudo nvidia-smi mig -i 0 -lgipp

GPU 0 Profile ID 19 Placements: {0, 1,2, 3, 4, 5, 6}:1
GPU 0 Profile ID 14 Placements: {0,2,4}:2
GPU 0 Profile ID  9 Placements: {0,4}:4
GPU 0 Profile ID  5 Placements: {0}:4
GPU 0 Profile ID  0 Placements: {0}:8

Step (2)
Create GPU Instances (GI) with ‘cgi’ option, for example, to split GPU 0 into 2 instances with 20 GB memory for each, use

Where the parameter to create GPU instance ‘cgi’ is the gpu instance profile id in column 3 above. The name of the instance like
‘MIG 3g.20gb’ can also be used instead. The parameter ‘cgi’ can be used to list the list of GPU instances

$ sudo nvidia-smi mig -i 0 -cgi 9,9

Successfully created GPU instance on GPU 0 using profile 1
Successfully created GPU instance on GPU 0 using profile 1

Now list the gpu instances

$ sudo nvidia-smi mig  -i 0 -lgi

GPU instances:
GPU    Name                 Profile ID       Instance ID        Placement Start:Size
0         MIG 3g.20gb          9                     1                              0:4
0         MIG 3g.20gb          9                     2                              4:4

Next create Compute Instance (CI) with ‘cci’ parameter for each GPU instance

$ sudo nvidia-smi mig -i 0 -gi 1 -cci 0

Successfully created compute instance on GPU 0 GPU instance ID 1 using profile ID 0

$ sudo nvidia-smi mig -i 0 -gi 2 -cci 0

Successfully created compute instance on GPU 0 GPU instance ID 2 using profile ID 0

Verify if the compute instances are created with ‘ice’ option

$ sudo nvidia-smi mig -i 0 -lci

Compute instances:
GPU         GPU instance   Name                     Profile ID    Instance ID
0                     1                  MIG 1c.3g.20gb           0                 0
0                     2                  MIG 1c.3g.20gb           0                 0

Step (3) 

To un the codes in MIG mode, et the Unique ID for eachh MIG device with “-L” option to .nvidia-smi. The new format follows
This convention:  MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID>

$ sudo nvidia-smi -L

GPU 0: A100-SXM4-40GB (UUID: GPU-2fa5cf01-9520-1bc1-9956b-cc)
      MIG 1c.3g.20gb Device 0: (UUID: GPU-2fa5cf01-9520-1bc1-9956b-cc/1/0)
      MIG 1c.3g.20gb Device 1: (UUID: GPU-2fa5cf01-9520-1bc2-9956b-cc/2/0)
GPU 1: A100-SXM4-40GB (UUID: GPU-2fa5cf01-9520-1bc1-9956b-cc)
GPU 2: A100-SXM4-40GB (UUID: GPU-4fa5cf01-9520-1bc1-9956b-cc)
GPU 3: A100-SXM4-40GB (UUID: GPU-acccf01-9520-1bc1-9956b-cc)
GPU 4: A100-SXM4-40GB (UUID: GPU-999cf01-9520-1bc1-9956b-cc)

NVIDIA_VISIBLE_DEVICES supports the following formats to specify MIG devices:
MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID> when using R450 and R460 drivers or  MIG-<UUID> starting with R470 drivers.
GPUDeviceIndex>:<MIGDeviceIndex>

Use the UUID to CUDA_VISIBLE_DEVICES environment variables as CUDA_VISIBLE_DEVICES=UUID <application> &

For example , to run two apps myapp1.py and myapp2.py

CUDA_VISIBLE_DEVICES=GPU-2fa5cf01-9520-1bc1-9956b-cc/1/0   myapp1.py &
CUDA_VISIBLE_DEVICES=GPU-2fa5cf01-9520-1bc1-9956b-cc/2/0   myapp1.py &

Verify if both are running on two instances with ‘nvidia-smi’ under the process tab

Example pytorch

sudo docker run --gpus '"device=0:1"' \
    nvcr.io/nvidia/pytorch:20.11-py3 \
    /bin/bash -c 'cd /opt/pytorch/examples/upstream/mnist && python main.py'

$ sudo nvidia-smi -i 0

Check under MIG devices and Process command output

Step (4)

To destroy the instances, first start with compute instances (CI) then GPU instances (GI)

$ sudo nvidia-smi mig -i 0 -gi 1 -dci

Successfully destroyed compute instance on GPU 0 GPU instance ID 1 using profile ID 0

$ sudo nvidia-smi mig -i 0 -gi 2 -dci

Successfully destroyed compute instance on GPU 0 GPU instance ID 2 using profile ID 0

If the intention is to destroy all the CIs and GIs, then this can be accomplished with the following commands:

$ sudo nvidia-smi mig -dci && sudo nvidia-smi mig -dgi

Verify all List MIG devices are deleted

$ sudo nvidia-smi -i 0

MIG devices should not appear anymore on device 0
