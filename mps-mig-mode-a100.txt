Argonne MPS/MIG Mode on A100
https://www.youtube.com/watch?v=nYI0yt3Q82Y&list=PLcbxjEfgjpO8HFNS3SHeTCY9bz9SkMNQs&index=5

https://docs.nvidia.com/datacenter/testla/mig-user-guide
https://github.com/aymericdamien/Tensorflow-Examples  Tensorflow Examples

Step (0)

Stop all running process using the GPU

By default MIG mode is not enabled on the GPU

$ nvidia-smi -i 0
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-SXM4-40GB      Off  | 00000000:36:00.0 Off |                    0 |
| N/A   29C    P0    62W / 400W |      0MiB / 40537MiB |      6%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+

MIG mode can be enabled on a GPU with the command

$ sudo nvidia-smi -i 0 -mig 1
Enabled MIG Mode for GPU 00000000:36:00.0
All done.

$ nvidia-smi -i 0 --query-gpu=pci.bus_id,mig.mode.current --format=csv
pci.bus_id, mig.mode.current
00000000:36:00.0, Enabled

List all enabled MIG GPU devices

$ nvidia-smi \
--query-gpu=pci.bus_id,mig.mode.current \
--format=csv
pci.bus_id, mig.mode.current
00000000:36:00.0, Enabled
00000000:3B:00.0, Enabled
00000000:41:00.0, Enabled
00000000:45:00.0, Enabled
00000000:59:00.0, Enabled
00000000:5D:00.0, Enabled
00000000:63:00.0, Enabled
00000000:67:00.0, Enabled


Step (1)

$ nvidia-smi 

Check if MIG mode is enabled with Nvidia-Smh this should list the GPUs followed by a table of MIG devices.
To see the list of possible GPU instances, use (what partitions I can do)

$ nvidia-smi mig -lgip

For a specific GPU, pass its id with -i option for GPU 0 use. (what partitions I can do)

$ nvidia-smi mig -i 0 -lgip

GPU instance profiles:
GPU     Name               ID    Instances    Memory         P2P     SM CE    DEC JPEG.  ENC OFA 
                                 Free/Total   GiB.
0	      MIG. 1g.5gb	       19	       7/7	    4.75		        No 	    14/1		0/0		      0/0
0	      MIG. 2g.10gb	     14	     3/3	      9.75		        No   	  28/2		1/0		      0/0
0	      MIG. 3g.20gb	     9	     2/2	      19.62	          No     	42/3		2/0		      0/0
0	      MIG. 4g.20gb	     5	     1/1	      19.62	          No 	    56/4		2/0		      0/0
0	      MIG. 7g.40gb	     0	     1/1	      39.50	          No 	    98/7		5/1		      0/0
                                                                         
Placements in how many ways you can split your gpu (See the placements choices use lgipp option

$ nvidia-smi mig -i 0 -lgipp

GPU 0 Profile ID 19 Placements: {0, 1,2, 3, 4, 5, 6}:1
GPU 0 Profile ID 14 Placements: {0,2,4}:2
GPU 0 Profile ID  9 Placements: {0,4}:4
GPU 0 Profile ID  5 Placements: {0}:4
GPU 0 Profile ID  0 Placements: {0}:8

Step (2)
Create GPU Instances (GI) with ‘cgi’ option, for example, to split GPU 0 into 2 instances with 20 GB memory for each, use

Where the parameter to create GPU instance ‘cgi’ is the gpu instance profile id in column 3 above. The name of the instance like
‘MIG 3g.20gb’ can also be used instead. The parameter ‘cgi’ can be used to list the list of GPU instances

$ nvidia-smi mig -i 0 -cgi 9,9

Successfully created GPU instance on GPU 0 using profile 1
Successfully created GPU instance on GPU 0 using profile 1

Now list the gpu instances

$ nvidia-smi mig  -i 0 -lgi

GPU instances:
GPU    Name                 Profile ID       Instance ID        Placement Start:Size
0         MIG 3g.20gb          9                     1                              0:4
0         MIG 3g.20gb          9                     2                              4:4

Next create Compute Instance (CI) with ‘cci’ parameter for each GPU instance

$ nvidia-smi mig -i 0 -gi 1 -cci 0

Successfully created compute instance on GPU 0 GPU instance ID 1 using profile ID 0

$ nvidia-smi mig -i 0 -gi 2 -cci 0

Successfully created compute instance on GPU 0 GPU instance ID 2 using profile ID 0

Verify if the compute instances are created with ‘ice’ option

$ nvidia-smi mig -i 0 -lci

Compute instances:
GPU         GPU instance   Name                     Profile ID    Instance ID
0                     1                  MIG 1c.3g.20gb           0                 0
0                     2                  MIG 1c.3g.20gb           0                 0

Step (3) 

To un the codes in MIG mode, et the Unique ID for eachh MIG device with “-L” option to .nvidia-smi. The new format follows
This convention:  MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID>

$ nvidia-smi -L

GPU 0: A100-SXM4-40GB (UUID: GPU-2fa5cf01-9520-1bc1-9956b-cc)
      MIG 1c.3g.20gb Device 0: (UUID: GPU-2fa5cf01-9520-1bc1-9956b-cc/1/0)
      MIG 1c.3g.20gb Device 1: (UUID: GPU-2fa5cf01-9520-1bc2-9956b-cc/2/0)
GPU 1: A100-SXM4-40GB (UUID: GPU-2fa5cf01-9520-1bc1-9956b-cc)
GPU 2: A100-SXM4-40GB (UUID: GPU-4fa5cf01-9520-1bc1-9956b-cc)
GPU 3: A100-SXM4-40GB (UUID: GPU-acccf01-9520-1bc1-9956b-cc)
GPU 4: A100-SXM4-40GB (UUID: GPU-999cf01-9520-1bc1-9956b-cc)

Use the UUID to CUDA_VISIBLE_DEVICES environment variables as CUDA_VISIBLE_DEVICES=UUID <application> &

For example , to run two apps myapp1.py and myapp2.py

CUDA_VISIBLE_DEVICES=GPU-2fa5cf01-9520-1bc1-9956b-cc/1/0   myapp1.py &
CUDA_VISIBLE_DEVICES=GPU-2fa5cf01-9520-1bc1-9956b-cc/2/0   myapp1.py &

Verify if both are running on two instances with ‘nvidia-smi’ under the process tab

$ nvidia-smi -i 0

Check under MIG devices and Process command output

Step (4)

To destroy the instances, first start with compute instances (CI) then GPU instances (GI)

$ nvidia-smi mig -i 0 -gi 1 -dci

Successfully destroyed compute instance on GPU 0 GPU instance ID 1 using profile ID 0

$ nvidia-smi mig -i 0 -gi 2 -dci

Successfully destroyed compute instance on GPU 0 GPU instance ID 2 using profile ID 0

$ nvidia-smi -i 0

MIG devices should not appear anymore on device 0
