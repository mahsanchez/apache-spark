Monitor
DCGM can get utilization metrics of mig device according to nv.

// Install dcgmi as documented at https://github.com/NVIDIA/DCGM

$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
$ sudo dpkg -i cuda-keyring_1.0-1_all.deb
$ sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"
$ sudo apt-get update && sudo apt-get install -y datacenter-gpu-manager
$ sudo systemctl --now enable nvidia-dcgm

// Create MIG groups
$ sudo nvidia-smi mig -cgi 9,9 -C

...
+-----------------------------------------------------------------------------+
| MIG devices:                                                                |
+------------------+----------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |
|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|
|                  |                      |        ECC|                       |
|==================+======================+===========+=======================|
|  0    1   0   0  |     11MiB / 20096MiB | 42      0 |  3   0    2    0    0 |
|                  |      0MiB / 32767MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0    2   0   1  |     11MiB / 20096MiB | 42      0 |  3   0    2    0    0 |
|                  |      0MiB / 32767MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+

// Discover MIG devices
$ dcgmi discovery -c

+-------------------+--------------------------------------------------------------------+
| Instance Hierarchy                                                                     |
+===================+====================================================================+
| GPU 0             | GPU GPU-5fd15f35-e148-2992-4ecb-9825e534f253 (EntityID: 0)         |
| -> I 0/1          | GPU Instance (EntityID: 0)                                         |
|    -> CI 0/1/0    | Compute Instance (EntityID: 0)                                     |
| -> I 0/2          | GPU Instance (EntityID: 1)                                         |
|    -> CI 0/2/0    | Compute Instance (EntityID: 1)                                     |
+-------------------+--------------------------------------------------------------------+

//we will add the GPU 0 and the two GPU Instances (using entity IDs 0 and 1) to the group:
$ dcgmi group -c mig-ex1 -a 0,i:0,i:1

Successfully created group "mig-ex1" with a group ID of 8
Specified entity ID is valid but unknown: i:0. ParsedResult: ParsedGpu(i:0)
Specified entity ID is valid but unknown: i:1. ParsedResult: ParsedGpu(i:1)
Add to group operation successful.

// list the devices in the group 
$  dcgmi group -l

+-------------------+----------------------------------------------------------+
| GROUPS                                                                       |
| 1 group found.                                                               |
+===================+==========================================================+
| Groups            |                                                          |
| -> 8              |                                                          |
|    -> Group ID    | 8                                                        |
|    -> Group Name  | mig-ex1                                                  |
|    -> Entities    | GPU 0, GPU_I 0, GPU_I 1                                  |
+-------------------+----------------------------------------------------------+

// run a workload
$ sudo dcgmproftester11 --no-dcgm-validation -t 1004 -d 120

// stream Memory Utilization ( DCGM_FI_DEV_VGPU_MEMORY_USAGE = 525 ) metrics with dcgmi dmon, group number appear   
$ dcgmi dmon -e 525 -g 8

Entity  GRACT  TENSO
      Id
    GPU 0  0.000  0.000
  GPU-I 0  0.000  0.000
  GPU-I 1  0.000  0.000
    GPU 0  0.000  0.000
  GPU-I 0  0.000  0.000
  GPU-I 1  0.000  0.000
    GPU 0  0.457  0.442
  GPU-I 0  0.534  0.516
  GPU-I 1  0.533  0.515
    GPU 0  0.845  0.816
  GPU-I 0  0.986  0.953
  GPU-I 1  0.985  0.952
    GPU 0  0.846  0.817
  GPU-I 0  0.987  0.953

// stream (DCGM_FI_DEV_VGPU_MEMORY_USAGE, SM Occupancy, SM Activity )  metrics with dcgmi dmon
$ dcgmi dmon -e 525,1003,1002 -g 8

// the total gpu utilization depends on the defined partitions
$ sudo nvidia-smi mig -lgip

// More Monitoring metrics can be found at https://docs.nvidia.com/datacenter/dcgm/1.7/dcgm-api/group__dcgmFieldIdentifiers.html

// Standalone GPU Devices (No MIG)
$ dcgmi discovery --host 127.0.0.1 -l
$ dcgmi dmon -e 525


Reference
https://github.com/NVIDIA/DCGM
https://docs.nvidia.com/datacenter/dcgm/1.7/dcgm-api/group__dcgmFieldIdentifiers.html
https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html#profiling-metrics
https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html
https://github.com/NVIDIA/gpu-monitoring-tools/issues/151
